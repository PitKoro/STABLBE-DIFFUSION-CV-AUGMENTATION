{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3de71460-6d5c-4562-a699-c3d27a398f1e",
   "metadata": {},
   "source": [
    "# Экпериментальный метод аугментации данных при помощи модели Stable Diffusion\n",
    "# (Еще не тестил)\n",
    "\n",
    "Предварительно в label-studio разметили области в которых будем генерировать дефекты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a553d25-2246-476f-87f1-7f7c194fcac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/src\n"
     ]
    }
   ],
   "source": [
    "%cd /app/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6b228b3-1fd0-40a5-aba8-53fa1da31fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: module 'triton.language' has no attribute 'constexpr'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling and loading c extensions from \"/app/src/PyPatchMatch\".\n",
      "rm -rf build/obj libpatchmatch.so\n",
      "mkdir: created directory 'build/obj'\n",
      "mkdir: created directory 'build/obj/csrc/'\n",
      "[dep] csrc/masked_image.cpp ...\n",
      "[dep] csrc/pyinterface.cpp ...\n",
      "[dep] csrc/nnf.cpp ...\n",
      "[dep] csrc/inpaint.cpp ...\n",
      "[CC] csrc/inpaint.cpp ...\n",
      "[CC] csrc/nnf.cpp ...\n",
      "[CC] csrc/pyinterface.cpp ...\n",
      "[CC] csrc/masked_image.cpp ...\n",
      "[link] libpatchmatch.so ...\n",
      "[Taichi] version 1.3.0, llvm 15.0.4, commit 0f25b95e, linux, python 3.7.13\n",
      "[I 12/20/22 18:01:07.812 4224] [shell.py:_shell_pop_print@33] Graphical python shell detected, using wrapped sys.stdout\n",
      "[PIE]Successfully initialize PIE grid solver with cuda backend\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os.path as osp\n",
    "# import pip\n",
    "# pip.main([\"install\",\"-v\",\"-U\",\"git+https://github.com/facebookresearch/xformers.git@main#egg=xformers\"])\n",
    "# subprocess.check_call(\"pip install -v -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers\", cwd=osp.dirname(__file__), shell=True)\n",
    "\n",
    "import io\n",
    "import base64\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import autocast\n",
    "import diffusers\n",
    "from diffusers.configuration_utils import FrozenDict\n",
    "from diffusers import (\n",
    "    StableDiffusionPipeline,\n",
    "    StableDiffusionInpaintPipeline,\n",
    "    StableDiffusionImg2ImgPipeline,\n",
    "    StableDiffusionInpaintPipelineLegacy,\n",
    "    DDIMScheduler,\n",
    "    LMSDiscreteScheduler,\n",
    "    StableDiffusionUpscalePipeline,\n",
    "    DPMSolverMultistepScheduler\n",
    ")\n",
    "from diffusers.models import AutoencoderKL\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "# import gradio as gr\n",
    "import base64\n",
    "import skimage\n",
    "import skimage.measure\n",
    "import yaml\n",
    "import json\n",
    "from enum import Enum\n",
    "\n",
    "try:\n",
    "    abspath = os.path.abspath(__file__)\n",
    "    dirname = os.path.dirname(abspath)\n",
    "    os.chdir(dirname)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from utils import *\n",
    "\n",
    "assert diffusers.__version__ >= \"0.6.0\", \"Please upgrade diffusers to 0.6.0\"\n",
    "\n",
    "USE_NEW_DIFFUSERS = True\n",
    "RUN_IN_SPACE = \"RUN_IN_HG_SPACE\" in os.environ\n",
    "\n",
    "\n",
    "class ModelChoice(Enum):\n",
    "    INPAINTING = \"stablediffusion-inpainting\"\n",
    "    INPAINTING_IMG2IMG = \"stablediffusion-inpainting+img2img-v1.5\"\n",
    "    MODEL_1_5 = \"stablediffusion-v1.5\"\n",
    "    MODEL_1_4 = \"stablediffusion-v1.4\"\n",
    "\n",
    "\n",
    "\n",
    "UnifiedPipeline = StableDiffusionInpaintPipeline\n",
    "\n",
    "\n",
    "\n",
    "USE_GLID = False\n",
    "\n",
    "\n",
    "try:\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "except:\n",
    "    cuda_available = False\n",
    "finally:\n",
    "    if sys.platform == \"darwin\":\n",
    "        device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    elif cuda_available:\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "\n",
    "import contextlib\n",
    "\n",
    "autocast = contextlib.nullcontext\n",
    "\n",
    "\n",
    "\n",
    "DEBUG_MODE = False\n",
    "\n",
    "try:\n",
    "    SAMPLING_MODE = Image.Resampling.LANCZOS\n",
    "except Exception as e:\n",
    "    SAMPLING_MODE = Image.LANCZOS\n",
    "\n",
    "try:\n",
    "    contain_func = ImageOps.contain\n",
    "except Exception as e:\n",
    "\n",
    "    def contain_func(image, size, method=SAMPLING_MODE):\n",
    "        # from PIL: https://pillow.readthedocs.io/en/stable/reference/ImageOps.html#PIL.ImageOps.contain\n",
    "        im_ratio = image.width / image.height\n",
    "        dest_ratio = size[0] / size[1]\n",
    "        if im_ratio != dest_ratio:\n",
    "            if im_ratio > dest_ratio:\n",
    "                new_height = int(image.height / image.width * size[0])\n",
    "                if new_height != size[1]:\n",
    "                    size = (size[0], new_height)\n",
    "            else:\n",
    "                new_width = int(image.width / image.height * size[1])\n",
    "                if new_width != size[0]:\n",
    "                    size = (new_width, size[1])\n",
    "        return image.resize(size, resample=method)\n",
    "\n",
    "\n",
    "\n",
    "model = {}\n",
    "\n",
    "\n",
    "def get_token():\n",
    "    token = \"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\"\n",
    "    return token\n",
    "\n",
    "\n",
    "def save_token(token):\n",
    "    with open(\".token\", \"w\") as f:\n",
    "        f.write(token)\n",
    "\n",
    "\n",
    "def prepare_scheduler(scheduler):\n",
    "    if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n",
    "        new_config = dict(scheduler.config)\n",
    "        new_config[\"steps_offset\"] = 1\n",
    "        scheduler._internal_dict = FrozenDict(new_config)\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "def my_resize(width, height):\n",
    "    if width >= 512 and height >= 512:\n",
    "        return width, height\n",
    "    if width == height:\n",
    "        return 512, 512\n",
    "    smaller = min(width, height)\n",
    "    larger = max(width, height)\n",
    "    if larger >= 608:\n",
    "        return width, height\n",
    "    factor = 1\n",
    "    if smaller < 290:\n",
    "        factor = 2\n",
    "    elif smaller < 330:\n",
    "        factor = 1.75\n",
    "    elif smaller < 384:\n",
    "        factor = 1.375\n",
    "    elif smaller < 400:\n",
    "        factor = 1.25\n",
    "    elif smaller < 450:\n",
    "        factor = 1.125\n",
    "    return int(factor * width)//8*8, int(factor * height)//8*8\n",
    "\n",
    "\n",
    "def load_learned_embed_in_clip(\n",
    "    learned_embeds_path, text_encoder, tokenizer, token=None\n",
    "):\n",
    "    # https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb\n",
    "    loaded_learned_embeds = torch.load(learned_embeds_path, map_location=\"cpu\")\n",
    "\n",
    "    # separate token and the embeds\n",
    "    trained_token = list(loaded_learned_embeds.keys())[0]\n",
    "    embeds = loaded_learned_embeds[trained_token]\n",
    "\n",
    "    # cast to dtype of text_encoder\n",
    "    dtype = text_encoder.get_input_embeddings().weight.dtype\n",
    "    embeds.to(dtype)\n",
    "\n",
    "    # add the token in tokenizer\n",
    "    token = token if token is not None else trained_token\n",
    "    num_added_tokens = tokenizer.add_tokens(token)\n",
    "    if num_added_tokens == 0:\n",
    "        raise ValueError(\n",
    "            f\"The tokenizer already contains the token {token}. Please pass a different `token` that is not already in the tokenizer.\"\n",
    "        )\n",
    "\n",
    "    # resize the token embeddings\n",
    "    text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # get the id for the token and assign the embeds\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    text_encoder.get_input_embeddings().weight.data[token_id] = embeds\n",
    "\n",
    "\n",
    "scheduler_dict = {\"PLMS\": None, \"DDIM\": None, \"K-LMS\": None, \"DPM\": None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bda0634-32eb-48f5-9742-7e8b0a0419d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_checkpoints = \"/app/checkpoints/sd-pipe-model\"\n",
    "\n",
    "class StableDiffusion:\n",
    "    def __init__(\n",
    "        self,\n",
    "        token: str = \"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\",\n",
    "        model_name: str = \"runwayml/stable-diffusion-v1-5\",\n",
    "        model_path: str = None,\n",
    "        inpainting_model: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.token = token\n",
    "        original_checkpoint = False\n",
    "        # vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\")\n",
    "        vae = AutoencoderKL.from_pretrained(f\"{my_checkpoints}/vae\")\n",
    "        vae.to(torch.float16)\n",
    "        if model_path and os.path.exists(model_path):\n",
    "            if model_path.endswith(\".ckpt\"):\n",
    "                original_checkpoint = True\n",
    "            elif model_path.endswith(\".json\"):\n",
    "                model_name = os.path.dirname(model_path)\n",
    "            else:\n",
    "                model_name = model_path\n",
    "        print(f'model_path == {model_path}')\n",
    "        if original_checkpoint:\n",
    "            print(f\"Converting & Loading {model_path}\")\n",
    "            from convert_checkpoint import convert_checkpoint\n",
    "\n",
    "            text2img = convert_checkpoint(model_path)\n",
    "            if device == \"cuda\":# and not args.fp32:\n",
    "                text2img.to(torch.float16)\n",
    "        else:\n",
    "            print(f\"Loading {model_name}\")\n",
    "            if device == \"cuda\":# and not args.fp32:\n",
    "                text2img = StableDiffusionPipeline.from_pretrained(\n",
    "                    my_checkpoints,\n",
    "                    revision=\"fp16\",\n",
    "                    torch_dtype=torch.float16,\n",
    "                    use_auth_token=token,\n",
    "                    vae=vae\n",
    "                )\n",
    "            else:\n",
    "                text2img = StableDiffusionPipeline.from_pretrained(\n",
    "                    model_name, use_auth_token=token,\n",
    "                )\n",
    "        if inpainting_model:\n",
    "            # can reduce vRAM by reusing models except unet\n",
    "            text2img_unet = text2img.unet\n",
    "            del text2img.vae\n",
    "            del text2img.text_encoder\n",
    "            del text2img.tokenizer\n",
    "            del text2img.scheduler\n",
    "            del text2img.safety_checker\n",
    "            del text2img.feature_extractor\n",
    "            import gc\n",
    "\n",
    "            gc.collect()\n",
    "            if device == \"cuda\":\n",
    "                inpaint = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "                    \"runwayml/stable-diffusion-inpainting\",\n",
    "                    revision=\"fp16\",\n",
    "                    torch_dtype=torch.float16,\n",
    "                    use_auth_token=token,\n",
    "                    vae=vae\n",
    "                ).to(device)\n",
    "            else:\n",
    "                inpaint = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "                    \"runwayml/stable-diffusion-inpainting\", use_auth_token=token,\n",
    "                ).to(device)\n",
    "            text2img_unet.to(device)\n",
    "            del text2img\n",
    "            gc.collect()\n",
    "            text2img = StableDiffusionPipeline(\n",
    "                vae=inpaint.vae,\n",
    "                text_encoder=inpaint.text_encoder,\n",
    "                tokenizer=inpaint.tokenizer,\n",
    "                unet=text2img_unet,\n",
    "                scheduler=inpaint.scheduler,\n",
    "                safety_checker=inpaint.safety_checker,\n",
    "                feature_extractor=inpaint.feature_extractor,\n",
    "            )\n",
    "        else:\n",
    "            inpaint = StableDiffusionInpaintPipelineLegacy(\n",
    "                vae=text2img.vae,\n",
    "                text_encoder=text2img.text_encoder,\n",
    "                tokenizer=text2img.tokenizer,\n",
    "                unet=text2img.unet,\n",
    "                scheduler=text2img.scheduler,\n",
    "                safety_checker=text2img.safety_checker,\n",
    "                feature_extractor=text2img.feature_extractor,\n",
    "            ).to(device)\n",
    "        text_encoder = text2img.text_encoder\n",
    "        tokenizer = text2img.tokenizer\n",
    "        if os.path.exists(\"./embeddings\"):\n",
    "            for item in os.listdir(\"./embeddings\"):\n",
    "                if item.endswith(\".bin\"):\n",
    "                    load_learned_embed_in_clip(\n",
    "                        os.path.join(\"./embeddings\", item),\n",
    "                        text2img.text_encoder,\n",
    "                        text2img.tokenizer,\n",
    "                    )\n",
    "        text2img.to(device)\n",
    "        if device == \"mps\":\n",
    "            _ = text2img(\"\", num_inference_steps=1)\n",
    "        scheduler_dict[\"PLMS\"] = text2img.scheduler\n",
    "        scheduler_dict[\"DDIM\"] = prepare_scheduler(\n",
    "            DDIMScheduler(\n",
    "                beta_start=0.00085,\n",
    "                beta_end=0.012,\n",
    "                beta_schedule=\"scaled_linear\",\n",
    "                clip_sample=False,\n",
    "                set_alpha_to_one=False,\n",
    "            )\n",
    "        )\n",
    "        scheduler_dict[\"K-LMS\"] = prepare_scheduler(\n",
    "            LMSDiscreteScheduler(\n",
    "                beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\"\n",
    "            )\n",
    "        )\n",
    "        scheduler_dict[\"DPM\"] = prepare_scheduler(\n",
    "            DPMSolverMultistepScheduler.from_config(text2img.scheduler.config)\n",
    "        )\n",
    "        self.safety_checker = text2img.safety_checker\n",
    "        img2img = StableDiffusionImg2ImgPipeline(\n",
    "            vae=text2img.vae,\n",
    "            text_encoder=text2img.text_encoder,\n",
    "            tokenizer=text2img.tokenizer,\n",
    "            unet=text2img.unet,\n",
    "            scheduler=text2img.scheduler,\n",
    "            safety_checker=text2img.safety_checker,\n",
    "            feature_extractor=text2img.feature_extractor,\n",
    "        ).to(device)\n",
    "        save_token(token)\n",
    "        try:\n",
    "            total_memory = torch.cuda.get_device_properties(0).total_memory // (\n",
    "                1024 ** 3\n",
    "            )\n",
    "            if total_memory <= 5:\n",
    "                inpaint.enable_attention_slicing()\n",
    "        except:\n",
    "            pass\n",
    "        self.text2img = text2img\n",
    "        self.inpaint = inpaint\n",
    "        self.img2img = img2img\n",
    "        self.unified = UnifiedPipeline(\n",
    "            vae=text2img.vae,\n",
    "            text_encoder=text2img.text_encoder,\n",
    "            tokenizer=text2img.tokenizer,\n",
    "            unet=text2img.unet,\n",
    "            scheduler=text2img.scheduler,\n",
    "            safety_checker=text2img.safety_checker,\n",
    "            feature_extractor=text2img.feature_extractor,\n",
    "        ).to(device)\n",
    "        self.inpainting_model = inpainting_model\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        image_pil,\n",
    "        prompt=\"\",\n",
    "        negative_prompt=\"\",\n",
    "        guidance_scale=7.5,\n",
    "        resize_check=True,\n",
    "        enable_safety=True,\n",
    "        fill_mode=\"patchmatch\",\n",
    "        strength=0.75,\n",
    "        step=50,\n",
    "        enable_img2img=False,\n",
    "        use_seed=False,\n",
    "        seed_val=-1,\n",
    "        generate_num=1,\n",
    "        scheduler=\"\",\n",
    "        scheduler_eta=0.0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        text2img, inpaint, img2img, unified = (\n",
    "            self.text2img,\n",
    "            self.inpaint,\n",
    "            self.img2img,\n",
    "            self.unified,\n",
    "        )\n",
    "        selected_scheduler = scheduler_dict.get(scheduler, scheduler_dict[\"PLMS\"])\n",
    "        for item in [text2img, inpaint, img2img, unified]:\n",
    "            item.scheduler = selected_scheduler\n",
    "            if enable_safety:\n",
    "                item.safety_checker = self.safety_checker\n",
    "            else:\n",
    "                item.safety_checker = lambda images, **kwargs: (images, False)\n",
    "        if RUN_IN_SPACE:\n",
    "            step = max(150, step)\n",
    "            image_pil = contain_func(image_pil, (1024, 1024))\n",
    "        width, height = image_pil.size\n",
    "        sel_buffer = np.array(image_pil)\n",
    "        img = sel_buffer[:, :, 0:3]\n",
    "        mask = sel_buffer[:, :, -1]\n",
    "        nmask = 255 - mask\n",
    "        process_width = width\n",
    "        process_height = height\n",
    "        if resize_check:\n",
    "            process_width, process_height = my_resize(width, height)\n",
    "        extra_kwargs = {\n",
    "            \"num_inference_steps\": step,\n",
    "            \"guidance_scale\": guidance_scale,\n",
    "            \"eta\": scheduler_eta,\n",
    "        }\n",
    "        if RUN_IN_SPACE:\n",
    "            generate_num = max(\n",
    "                int(4 * 512 * 512 // process_width // process_height), generate_num\n",
    "            )\n",
    "        if USE_NEW_DIFFUSERS:\n",
    "            extra_kwargs[\"negative_prompt\"] = negative_prompt\n",
    "            extra_kwargs[\"num_images_per_prompt\"] = generate_num\n",
    "        if use_seed:\n",
    "            generator = torch.Generator(text2img.device).manual_seed(seed_val)\n",
    "            extra_kwargs[\"generator\"] = generator\n",
    "        if nmask.sum() < 1 and enable_img2img:\n",
    "            init_image = Image.fromarray(img)\n",
    "            if True:\n",
    "                images = img2img(\n",
    "                    prompt=prompt,\n",
    "                    init_image=init_image.resize(\n",
    "                        (process_width, process_height), resample=SAMPLING_MODE\n",
    "                    ),\n",
    "                    strength=strength,\n",
    "                    **extra_kwargs,\n",
    "                )[\"images\"]\n",
    "        elif mask.sum() > 0:\n",
    "            if fill_mode == \"g_diffuser\" and not self.inpainting_model:\n",
    "                mask = 255 - mask\n",
    "                mask = mask[:, :, np.newaxis].repeat(3, axis=2)\n",
    "                img, mask, out_mask = functbl[fill_mode](img, mask)\n",
    "                extra_kwargs[\"strength\"] = 1.0\n",
    "                extra_kwargs[\"out_mask\"] = Image.fromarray(out_mask)\n",
    "                inpaint_func = unified\n",
    "            else:\n",
    "                img, mask = functbl[fill_mode](img, mask)\n",
    "                mask = 255 - mask\n",
    "                mask = skimage.measure.block_reduce(mask, (8, 8), np.max)\n",
    "                mask = mask.repeat(8, axis=0).repeat(8, axis=1)\n",
    "                extra_kwargs[\"strength\"] = strength\n",
    "                inpaint_func = inpaint\n",
    "            init_image = Image.fromarray(img)\n",
    "            mask_image = Image.fromarray(mask)\n",
    "            # mask_image=mask_image.filter(ImageFilter.GaussianBlur(radius = 8))\n",
    "            if True:\n",
    "                input_image = init_image.resize(\n",
    "                    (process_width, process_height), resample=SAMPLING_MODE\n",
    "                )\n",
    "                images = inpaint_func(\n",
    "                    prompt=prompt,\n",
    "                    init_image=input_image,\n",
    "                    image=input_image,\n",
    "                    width=process_width,\n",
    "                    height=process_height,\n",
    "                    mask_image=mask_image.resize((process_width, process_height)),\n",
    "                    **extra_kwargs,\n",
    "                )[\"images\"]\n",
    "        else:\n",
    "            if True:\n",
    "                images = text2img(\n",
    "                    prompt=prompt,\n",
    "                    height=process_width,\n",
    "                    width=process_height,\n",
    "                    **extra_kwargs,\n",
    "                )[\"images\"]\n",
    "        return images\n",
    "    \n",
    "def get_model(token=\"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\", model_choice=\"\", model_path=\"\"):\n",
    "    if \"model\" not in model:\n",
    "        model_name = \"\"\n",
    "        if model_choice == ModelChoice.INPAINTING.value:\n",
    "            if len(model_name) < 1:\n",
    "                model_name = \"runwayml/stable-diffusion-inpainting\"\n",
    "            print(f\"Using [{model_name}] {model_path}\")\n",
    "            tmp = StableDiffusionInpaint(\n",
    "                token=token, model_name=model_name, model_path=model_path\n",
    "            )\n",
    "        elif model_choice == ModelChoice.INPAINTING_IMG2IMG.value:\n",
    "            print(\n",
    "                f\"Note that {ModelChoice.INPAINTING_IMG2IMG.value} only support remote model and requires larger vRAM\"\n",
    "            )\n",
    "            tmp = StableDiffusion(token=token, model_name=\"runwayml/stable-diffusion-v1-5\", inpainting_model=True)\n",
    "        else:\n",
    "            if len(model_name) < 1:\n",
    "                model_name = (\n",
    "                    \"runwayml/stable-diffusion-v1-5\"\n",
    "                    if model_choice == ModelChoice.MODEL_1_5.value\n",
    "                    else \"CompVis/stable-diffusion-v1-4\"\n",
    "                )\n",
    "            tmp = StableDiffusion(\n",
    "                token=token, model_name=model_name, model_path=model_path\n",
    "            )\n",
    "        model[\"model\"] = tmp\n",
    "        print(f'model name={model_name}')\n",
    "    return model[\"model\"]\n",
    "    \n",
    "\n",
    "def run_outpaint(\n",
    "    img,\n",
    "    prompt_text,\n",
    "    negative_prompt_text,\n",
    "    strength,\n",
    "    guidance,\n",
    "    step,\n",
    "    resize_check,\n",
    "    fill_mode,\n",
    "    enable_safety,\n",
    "    use_correction,\n",
    "    enable_img2img,\n",
    "    use_seed,\n",
    "    seed_val,\n",
    "    generate_num,\n",
    "    scheduler,\n",
    "    scheduler_eta,\n",
    "    model_path=''\n",
    "):\n",
    "    width, height = img.size\n",
    "    pil = img\n",
    "    pil.convert('RGB')\n",
    "    sel_buffer = np.array(pil)\n",
    "    cur_model = get_model(model_path=model_path)\n",
    "    images = cur_model.run(\n",
    "        image_pil=pil,\n",
    "        prompt=prompt_text,\n",
    "        negative_prompt=negative_prompt_text,\n",
    "        guidance_scale=guidance,\n",
    "        strength=strength,\n",
    "        step=step,\n",
    "        resize_check=resize_check,\n",
    "        fill_mode=fill_mode,\n",
    "        enable_safety=enable_safety,\n",
    "        use_seed=use_seed,\n",
    "        seed_val=seed_val,\n",
    "        generate_num=generate_num,\n",
    "        scheduler=scheduler,\n",
    "        scheduler_eta=scheduler_eta,\n",
    "        enable_img2img=enable_img2img,\n",
    "        width=width,\n",
    "        height=height,\n",
    "    )\n",
    "    base64_str_lst = []\n",
    "    if enable_img2img:\n",
    "        use_correction = \"border_mode\"\n",
    "    for image in images:\n",
    "        image = correction_func.run(pil.resize(image.size), image, mode=use_correction)\n",
    "        resized_img = image.resize((width, height), resample=SAMPLING_MODE,)\n",
    "        out = sel_buffer.copy()\n",
    "        out[:, :, 0:3] = np.array(resized_img)\n",
    "        out[:, :, -1] = 255\n",
    "        out_pil = Image.fromarray(out)\n",
    "        return out_pil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7c9bae5-4198-4da7-a382-b3b49ae234c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cce56c20-7ff3-43e3-b765-a38f9e704daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = json.load(open('/app/dataset/result.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e00f905-c9fb-4571-8140-ea9263392f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': [{'bbox': [620, 335, 130, 139],\n",
       "   'file_name': '/app/dataset/images/6a8a2444-orig2.png'},\n",
       "  {'bbox': [257, 68, 191, 199],\n",
       "   'file_name': '/app/dataset/images/6a8a2444-orig2.png'},\n",
       "  {'bbox': [278, 562, 174, 183],\n",
       "   'file_name': '/app/dataset/images/6a8a2444-orig2.png'},\n",
       "  {'bbox': [864, 55, 169, 184],\n",
       "   'file_name': '/app/dataset/images/6a8a2444-orig2.png'},\n",
       "  {'bbox': [1180, 587, 137, 140],\n",
       "   'file_name': '/app/dataset/images/6a8a2444-orig2.png'},\n",
       "  {'bbox': [7, 15, 136, 155],\n",
       "   'file_name': '/app/dataset/images/6a8a2444-orig2.png'}],\n",
       " '1': [],\n",
       " '2': [],\n",
       " '3': [],\n",
       " '4': []}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_and_imgs_for_aug = {\n",
    "    \"0\" : [],\n",
    "    \"1\" : [],\n",
    "    \"2\" : [],\n",
    "    \"3\" : [],\n",
    "    \"4\" : [],\n",
    "}\n",
    "\n",
    "\n",
    "for ann_image in ann['images']:\n",
    "    img_id = ann_image['id']\n",
    "    img_name = ann_image['file_name']\n",
    "    \n",
    "    for ann_ann in ann['annotations']:\n",
    "        if ann_ann['image_id'] == img_id:\n",
    "            category_id = ann_ann['category_id']\n",
    "            categories_and_imgs_for_aug[str(category_id)].append(\n",
    "                {\n",
    "                    'bbox':ann_ann['bbox'],\n",
    "                    'file_name': img_name,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            \n",
    "categories_and_imgs_for_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678194b7-0f69-4a2a-bb00-734ca0c47c56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from chitra.image import Chitra\n",
    "\n",
    "\n",
    "aug_ann = dict()\n",
    "aug_ann['categories'] = ann['categories']\n",
    "aug_ann['info'] = ann['info']\n",
    "aug_ann['images'] = []\n",
    "aug_ann['annotations'] = []\n",
    "\n",
    "\n",
    "\n",
    "prompts_for_categories = {\n",
    "    \"0\" : 'gap on metal pipe', #Выход трубы из ряда\n",
    "    \"1\" : 'with stablediffusionn crack', #Крип\n",
    "    \"2\" : 'with stablediffusionn crack', #Разрыв\n",
    "    \"3\" : 'black corrosioAugmentation with stablediffusionn crack on old metal pipe', #Свищ\n",
    "    \"4\" : 'a long black crack on an old pipe located vertically', #Трещина\n",
    "}\n",
    "\n",
    "\n",
    "i=0\n",
    "image_id = 0\n",
    "annotaion_id = 0\n",
    "for category_id, annotations in categories_and_imgs_for_aug.items():\n",
    "    if(len(annotations) != 0):\n",
    "        for annotation in annotations:\n",
    "            print(annotations)\n",
    "\n",
    "            img_path = annotation['file_name']\n",
    "            img_name = os.path.basename(img_path)\n",
    "            img_name_without_ext = img_name.split('.')[0]\n",
    "            path_to_save_cropped_img = f\"/app/img/cropped/{img_name_without_ext}_crop_category{str(category_id)}_{i}.png\"\n",
    "            img = Image.open(img_path).convert('RGBA')\n",
    "            original_size = img.size\n",
    "            # img = img.resize((1380,800))\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            bbox = annotation['bbox']\n",
    "            x0, y0 = (bbox[0], bbox[1])\n",
    "            x1, y1 = (x0 + bbox[2], y0 + bbox[3])\n",
    "            draw.rectangle([x0, y0, x1, y1], fill=(0,0,0,0)) #[(x0, y0), (x1, y1)]\n",
    "            img = img.resize((1380,800))\n",
    "            img.save(path_to_save_cropped_img)\n",
    "            \n",
    "            for j in range(10):\n",
    "                path_to_save_auged_img = f\"/app/img/augs/{img_name_without_ext}_aug_category{str(category_id)}_{i}_{j}.png\"\n",
    "                img = Image.open(path_to_save_cropped_img)\n",
    "                print(f\"predicted img: {path_to_save_cropped_img}\")\n",
    "                w,h = img.size\n",
    "                print(f\"positive prompt: {prompts_for_categories[str(category_id)]}\")\n",
    "                new_img = run_outpaint(\n",
    "                    img=img,\n",
    "                    prompt_text=prompts_for_categories[str(category_id)],\n",
    "                    negative_prompt_text='',\n",
    "                    strength=1,\n",
    "                    guidance=12,\n",
    "                    step=20,\n",
    "                    resize_check=True,\n",
    "                    fill_mode='cv2_ns',\n",
    "                    enable_safety=False,\n",
    "                    use_correction=\"disabled\",\n",
    "                    enable_img2img=False,\n",
    "                    use_seed=False,\n",
    "                    seed_val=0,\n",
    "                    generate_num=1,\n",
    "                    scheduler='DPM',\n",
    "                    scheduler_eta=0,\n",
    "                    model_path='/app/checkpoints/sd-pipe-model'\n",
    "                )\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                print(f\"img.size={img.size}\")\n",
    "                print(f\"new_img.size={new_img.size}\")\n",
    "                new_img.save(path_to_save_auged_img)\n",
    "\n",
    "                print(\"FOR COCO-------------------------\")\n",
    "\n",
    "                chitra_image = Chitra(img_path, [x0, y0, x1, y1], 'pipe')\n",
    "                chitra_image.resize_image_with_bbox((1380,800))\n",
    "                chitra_bbox = chitra_image.bboxes[0]\n",
    "                new_x0, new_y0, new_x1, new_y1 = (chitra_bbox.x1, chitra_bbox.y1, chitra_bbox.x2, chitra_bbox.y2)\n",
    "                new_bbox = [new_x0, new_y0, new_x1 - new_x0, new_y1 - new_y0]\n",
    "\n",
    "                aug_ann['images'].append(\n",
    "                    {\n",
    "                        \"width\": w,\n",
    "                        \"height\": h,\n",
    "                        \"id\": image_id,\n",
    "                        \"file_name\": os.path.basename(path_to_save_auged_img)\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                aug_ann['annotations'].append(\n",
    "                    {\n",
    "                        \"id\": annotaion_id,\n",
    "                        \"image_id\": image_id,\n",
    "                        \"category_id\": int(category_id),\n",
    "                        \"segmentation\": [],\n",
    "                        \"bbox\": [\n",
    "                          int(new_bbox[0]),\n",
    "                          int(new_bbox[1]),\n",
    "                          math.ceil(new_bbox[2]),\n",
    "                          math.ceil(new_bbox[3])\n",
    "                        ],\n",
    "                        \"ignore\": 0,\n",
    "                        \"iscrowd\": 0,\n",
    "                        \"area\": math.ceil(new_bbox[2]) * math.ceil(new_bbox[3])\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                i += 1\n",
    "                image_id += 1\n",
    "                annotaion_id += 1\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe512aa-5a6d-4fc9-aac6-f0b7c3a01c28",
   "metadata": {},
   "source": [
    "Сохраняем аннотации в формате coco:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "539f4898-7424-40bf-add5-ee6e3fe57a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../img/test_stable_aug_ann_with_three_imgs.json\", 'w') as f:\n",
    "    json.dump(aug_ann, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0bb12b-f5d0-4215-a159-b4ea65e4b4dd",
   "metadata": {},
   "source": [
    "### COCO to label-studio format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72bd9dd9-3c07-4d72-a8ae-fed73c17508a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'label-studio-converter'...\n",
      "remote: Enumerating objects: 1496, done.\u001b[K\n",
      "remote: Counting objects: 100% (779/779), done.\u001b[K\n",
      "remote: Compressing objects: 100% (320/320), done.\u001b[K\n",
      "remote: Total 1496 (delta 562), reused 583 (delta 457), pack-reused 717\u001b[K\n",
      "Receiving objects: 100% (1496/1496), 2.66 MiB | 8.06 MiB/s, done.\n",
      "Resolving deltas: 100% (845/845), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/heartexlabs/label-studio-converter.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b084712e-dd38-4103-9c70-5ed3aade762f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///app/src/label-studio-converter\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.24.0 in /opt/conda/lib/python3.7/site-packages (from label-studio-converter==0.0.48.dev0) (1.3.5)\n",
      "Requirement already satisfied: requests<3,>=2.22.0 in /opt/conda/lib/python3.7/site-packages (from label-studio-converter==0.0.48.dev0) (2.28.1)\n",
      "Collecting Pillow==9.3.0\n",
      "  Downloading Pillow-9.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nltk==3.6.7\n",
      "  Downloading nltk-3.6.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting label-studio-tools==0.0.1\n",
      "  Downloading label_studio_tools-0.0.1-py3-none-any.whl (10 kB)\n",
      "Collecting lxml>=4.2.5\n",
      "  Downloading lxml-4.9.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting appdirs>=1.4.3\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk==3.6.7->label-studio-converter==0.0.48.dev0) (2022.10.31)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk==3.6.7->label-studio-converter==0.0.48.dev0) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk==3.6.7->label-studio-converter==0.0.48.dev0) (4.64.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->label-studio-converter==0.0.48.dev0) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->label-studio-converter==0.0.48.dev0) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->label-studio-converter==0.0.48.dev0) (1.21.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.22.0->label-studio-converter==0.0.48.dev0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.22.0->label-studio-converter==0.0.48.dev0) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.22.0->label-studio-converter==0.0.48.dev0) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.22.0->label-studio-converter==0.0.48.dev0) (2.0.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->label-studio-converter==0.0.48.dev0) (1.16.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->nltk==3.6.7->label-studio-converter==0.0.48.dev0) (5.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk==3.6.7->label-studio-converter==0.0.48.dev0) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk==3.6.7->label-studio-converter==0.0.48.dev0) (4.1.1)\n",
      "Installing collected packages: appdirs, Pillow, lxml, joblib, label-studio-tools, nltk, label-studio-converter\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: Pillow 9.0.1\n",
      "    Uninstalling Pillow-9.0.1:\n",
      "      Successfully uninstalled Pillow-9.0.1\n",
      "  Running setup.py develop for label-studio-converter\n",
      "Successfully installed Pillow-9.3.0 appdirs-1.4.4 joblib-1.2.0 label-studio-converter-0.0.48.dev0 label-studio-tools-0.0.1 lxml-4.9.2 nltk-3.6.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -e ./label-studio-converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b71f7630-512b-4f73-b38b-0ef51bd2adeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Reading COCO notes and categories from /app/img/test_stable_aug_ann_with_three_imgs.json\n",
      "INFO:root:Found 1 categories, 60 images and 60 annotations\n",
      "WARNING:root:Segmentation in COCO is experimental\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/label-studio-converter\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('label-studio-converter', 'console_scripts', 'label-studio-converter')())\n",
      "  File \"/app/src/label-studio-converter/label_studio_converter/main.py\", line 132, in main\n",
      "    imports(args)\n",
      "  File \"/app/src/label-studio-converter/label_studio_converter/main.py\", line 122, in imports\n",
      "    image_root_url=args.image_root_url, point_width=args.point_width)\n",
      "  File \"/app/src/label-studio-converter/label_studio_converter/imports/coco.py\", line 198, in convert_coco_to_ls\n",
      "    item = create_segmentation(annotation, categories, segmentation_from_name, image_height, image_width, to_name)\n",
      "  File \"/app/src/label-studio-converter/label_studio_converter/imports/coco.py\", line 55, in create_segmentation\n",
      "    segmentation = annotation['segmentation'][0]\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "!label-studio-converter import coco -i /app/img/test_stable_aug_ann_with_three_imgs.json -o img/stable_aug_for_label_studio.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81faac1d-0d33-46ea-9e2b-4c63be83b749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
